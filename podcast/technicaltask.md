Prototype Prompt â€” Activity-Aware Podcast Recommender (Streamlit) + LLM Review Summaries Role You are a senior Python engineer, Streamlit expert, and LLM integration engineer. Goal Build a self-contained prototype of an activity-aware podcast recommender with: Explainability (â€œWhy this?â€ panel) Cold start handling Diversity control (MMR) Feedback loop (session-level learning) Basic telemetry to CSV LLM-based review summarization per podcast, using Together API The prototype must be optimized for a live demo and run end-to-end offline except for the explicit LLM calls to Together. Scope Build only what follows; exclude production infrastructure. Include Streamlit UI (mobile-first) In-memory dataset for episodes/shows Local data files: data/sample_podcasts.csv reviews.json database.db (local SQLite stub/auxiliary file; can be used for future extensions but no complex DB layer) TF-IDF + simple embeddings (scikit-learn) MMR-based diversification â€œWhy this?â€ explainability panel for ranking LLM-powered â€œReview summaryâ€ per podcast, based on reviews.json Feedback that updates ranking in session state Basic telemetry to CSV in project root or /tmp Exclude Auth Payments External APIs other than Together LLM for review summarization Real audio playback External/managed databases (only local database.db file within the repo is allowed; no network DBs) Background jobs Microservices Cloud deployment Deliverables (code & data) Code app.py Single-file Streamlit app is acceptable. Contains: UI Ranking model MMR diversification Feedback logic Telemetry logging LLM integration for review summaries Data files data/sample_podcasts.csv (â‰ˆ40â€“60 rows) Columns (header required): show_id show_title publisher tags language explicit (bool or 0/1) avg_len_min freq (e.g., â€œdailyâ€, â€œweeklyâ€) episode_id ep_title ep_desc ep_duration_min soft_start (bool; true if good for gentle/sleep start) publish_ts (ISO datetime string) popularity_score (0â€“1 float) reviews.json Line-delimited JSON objects with user reviews. Example format (one JSON object per line): {"podcast_id":"52e3d2c4fab4e80a8bb75ad144671d96","title":"Argh","content":"What the heck is this show? And why canâ€™t I delete it from my feed. It just wonâ€™t die!","rating":1,"author_id":"a23655ce5565f32","created_at":"2019-05-01 01:24:01+00"} {"podcast_id":"52e3d2c4fab4e80a8bb75ad144671d96","title":"Miss gross overrated as interviewer.","content":"The Howard stern interview was horrible. I learned nothing- she doesnâ€™t understand psychoanalysis so she couldnâ€™t ask appropriate questions about how itâ€™s helped him. Sheâ€™s not very smart.","rating":1,"author_id":"e5d461c5f91c8d2","created_at":"2019-06-07 14:25:34+00"} {"podcast_id":"52e3d2c4fab4e80a8bb75ad144671d96","title":"I love Terri and Fresh Air","content":"People get really pissy about this beautiful free content. Maybe they should go outside and get some literal fresh air. Thanks for everything NPR.","rating":5,"author_id":"806cc75442c8f3d","created_at":"2019-05-15 09:42:53+00"} podcast_id should correspond to show_id or episode_id (define and use consistently; recommended: map reviews to show_id). database.db Local SQLite file in project root. Keep usage minimal; it can store auxiliary data (e.g., cached features, mappings, or be unused but present). No migrations or heavy ORM needed; this is not a production DB. Telemetry telemetry.csv (auto-created if missing, in project root) Append-only logging of user interactions. How to run Provide clear instructions in README or at the top of app.py (3 steps): Install dependencies: pip install streamlit scikit-learn pandas numpy together Set Together API key via environment variable (do not hard-code in code): export TOGETHER_API_KEY="YOUR_API_KEY_HERE" Run the app: streamlit run app.py No placeholders â€” the app must run end-to-end with the provided CSV/JSON/DB files and the Together API key. Data & Features sample_podcasts.csv Generate a small, varied dataset (EN) across topics: True Crime, Comedy, Business, Tech, News, Sports, Self-help, History. tags: helpful tokens for ranking & activity matching, e.g.: â€œcalm, interview, background, energetic, storytelling, news, narrative, sleep, focusâ€. Mix of durations: ep_duration_min in the range 10â€“90 minutes. explicit: mix of explicit and non-explicit content. language: at least {en, es, ru}. popularity_score: range [0, 1] with variation across episodes and shows. soft_start: true for content suitable to start Sleep sessions gently. reviews.json Provide multiple reviews per podcast_id (mixed positive/negative, different ratings 1â€“5). Use coherent English text; can mix tones (angry, neutral, enthusiastic). created_at in timestamp format â€œYYYY-MM-DD HH:MM:SS+00â€. Keep author_id arbitrary (string). UI Requirements (Streamlit, mobile-first) Entry point Hero question: â€œWhat are you doing now?â€ Segmented control (radio or buttons): Commute â€¢ Workout â€¢ Focus â€¢ Sleep Workout sub-modes When Workout is selected, show sub-mode select: Run â€¢ Lift â€¢ Cardio Sub-mode influences target duration and energy tags in ActivityFit. Search Text input for interest-based query: Placeholder/example: â€œAI ethicsâ€, â€œPremier Leagueâ€, â€œStoicismâ€. Query is optional; empty query is allowed (should fall back to activity + interests). Filters Duration range slider (in minutes) Example default: [15, 60]. Language select (multi-select or single-select): At least {Any, en, es, ru}. â€œNew / Evergreenâ€ toggle: â€œNewâ€ = recent episodes (publish_ts within last 30 days). â€œEvergreenâ€ = no recency constraint. Diversity control Slider: â€œMore familiar â†” More exploratoryâ€ Maps to MMR Î» in [0, 1]. Default Î» = 0.5. Result cards (Top-10) For each recommended episode (up to Top-10): Art placeholder (emoji is enough). Title (ep_title). Publisher (show_title + publisher). Avg length (avg_len_min or ep_duration_min). Frequency badge (freq). Badges with short labels: Interest Activity Query Popularity (show numeric 0â€“1). 1-line reason text (â€œExplainability Textâ€). Controls: â€œâ–¶ Play latestâ€ (dummy click; log as play in telemetry). â€œSaveâ€ (toggles local saved state; log event). â€œğŸ‘ / ğŸ‘â€ feedback buttons. â€œâ‹¯ Not for this activityâ€ (down-weights for this activity only). â€œReview summaryâ€ button â†’ triggers LLM summarization of user reviews for this podcast. Small chevron or â€œWhy this?â€ link to expand explainability panel. Explainability panel (â€œWhy this?â€) On expand, show: Interest overlap terms (top 5 terms contributing to InterestFit). Activity match keywords (e.g., â€œcalmâ€, â€œstorytellingâ€, â€œenergeticâ€, â€œbackgroundâ€, etc.). Query hit tokens (terms from query that matched TF-IDF). â€œBecause you liked X/Yâ€: Use session history of liked/saved items to mention 1â€“2 show titles or tags driving similarity. Cold start behavior If no user history (no likes/saves; no explicit interests yet): Show chips (multi-select) for quick interest pick (10â€“15 chips): e.g., â€œTechâ€, â€œTrue Crimeâ€, â€œComedyâ€, â€œSelf-helpâ€, â€œFootballâ€, â€œNewsâ€, â€œHistoryâ€, â€œMindfulnessâ€, etc. Microcopy: â€œNo history yet â€” tuned for your activity.â€ After selecting chips: Build a simple interest vector from tags/topics. Provide activity-only + interest-based list with high diversity (lower Î», e.g., 0.4â€“0.5). Safety & comfort Sleep + Focus: Hide explicit episodes by default. Prefer soft_start = true for Sleep. Focus: Down-weight Comedy/high-energy tags (e.g., â€œcomedyâ€, â€œenergeticâ€, â€œhypeâ€). Workout: Allow more energetic + high-intensity tags. Sub-mode: Run: prefer continuous narrative or music 30â€“60 min. Lift: allow slightly longer, high-energy episodes. Cardio: moderate durations, energetic tags. Accessibility: Keyboard-first navigation where supported by Streamlit. Use high contrast in layout. Provide descriptive labels and aria-like hints via Streamlit component labels and help texts. Ranking Model (implemented in code) Text features Use scikit-learn: TF-IDF over (ep_title + ep_desc + tags). Cache TF-IDF model and vectors via st.cache_data or equivalent. Signals (all normalized 0..1) InterestFit Cosine similarity between episode TF-IDF vector and a user interest vector. User interest vector built from: Selected chips (topics). Liked episodes (ğŸ‘ / saved items). Increased weights for terms around items with positive feedback. ActivityFit Duration fitness: Target window depends on activity and Workout sub-mode. Smooth score in [0, 1], with 1 for durations inside preferred window; penalize outside. Tag bias: Sleep â†’ prefer {â€œcalmâ€, â€œstorytellingâ€, â€œsoft_startâ€}. Focus â†’ prefer {â€œinterviewâ€, â€œbackgroundâ€, â€œdeepâ€, â€œlow-energyâ€}. Workout â†’ prefer {â€œenergeticâ€, â€œupbeatâ€, â€œmotivationalâ€}. Commute â†’ prefer {â€œnarrativeâ€, â€œnewsâ€, â€œdaily briefâ€}. QueryMatch TF-IDF cosine similarity between episode and query text. If query empty, QueryMatch = 0. BM25-style behavior can be approximated via TF-IDF scoring; no separate library required. Popularity Use popularity_score from CSV, normalized to [0, 1]. Freshness boost: If publish_ts is recent (e.g., last 30 days), add +0.05, clipped to 1.0. DurationFit (explicit) Penalty for being outside user-selected duration range slider. Smoothly decrease score as distance from range grows. Pre-MMR relevance Compute: relevance = wIInterestFit + wAActivityFit + wQQueryMatch + wPPopularity + wD*DurationFit Default weights: wI = 0.35 wA = 0.25 wQ = 0.20 wP = 0.15 wD = 0.05 Allow minor per-activity tweaks: Sleep: slightly lower Î» (more diversity penalty) and higher ActivityFit importance. Workout: higher wA and wP may be acceptable. MMR diversification Use TF-IDF vectors for similarity. For a candidate item i and selected set S: final_score(i) = Î» * relevance(i) - (1 - Î») * max_sim(i, S) Î» is controlled by the diversity slider in UI (0..1, default 0.5). Build Top-10 with greedy selection using MMR. Guardrails No more than 2 episodes from the same show in Top-5. If Î» â‰¥ 0.5: Ensure at least 1 novel item (not previously shown or interacted with in this session). Feedback & session-level learning Feedback controls ğŸ‘: Increase user interest weights around this itemâ€™s top TF-IDF terms. Optionally boost show-level prior for future episodes from same show. ğŸ‘: Decrease interest weights for this itemâ€™s top terms. Show a reason prompt (radio or selectbox): â€œToo longâ€ â€œNot my topicâ€ â€œToo intense for sleepâ€ â€œIâ€™ve heard itâ€ Update: â€œToo longâ€ â†’ shift preferred duration window shorter. â€œNot my topicâ€ â†’ down-weight tags/topics present in this episode. â€œToo intense for sleepâ€ â†’ penalize high-energy/comedy tags for Sleep. â€œIâ€™ve heard itâ€ â†’ down-rank this episode specifically and similar episodes. â€œNot for this activityâ€: Maintain a per-activity penalty for this show (show_id) in session state. Do not globally ban the show; just down-weight when this activity is selected. Behavior All feedback updates must immediately impact ranking in the same session, without page reload (i.e., using Streamlit session_state and reactive rerun). LLM-based review summarization (Together API) Goal For each podcast (show or episode, pick one consistently), provide a â€œReview summaryâ€ button. When clicked: Load reviews for that podcast_id from reviews.json. Call an LLM via Together API to produce a short, readable summary of user opinions: Overall sentiment Common pros/cons Typical themes (e.g., â€œgreat interviewerâ€, â€œtoo slowâ€, â€œconfusingâ€, etc.) Display summary inline on the card or in an expandable area. LLM provider & model Use Together Python client: from together import Together client = Together() response = client.chat.completions.create( model="meta-llama/Llama-3.2-3B-Instruct-Turbo", messages=[ {"role": "user", "content": "What are some fun things to do in New York?"} ] ) print(response.choices[0].message.content) Adapt this example to: Build a prompt using the selected podcastâ€™s reviews. Ask the model to summarize reviews. API key handling Use Together API via api.together.* (Python client). API key must be provided via environment variable (e.g., TOGETHER_API_KEY) or configuration file excluded from version control. Do not hard-code the API key in app.py or in the prompt text. Review summarization behavior Implement helper function, e.g.: summarize_reviews(podcast_id: str) -> str Steps: Filter lines from reviews.json by podcast_id. If no reviews: Return a simple message: â€œNo user reviews available yet.â€ If too many reviews: Sample or truncate to reasonable count (e.g., up to 20â€“30 most recent by created_at). Construct an LLM prompt: Include: Podcast title (if available from CSV). Short list of reviews (title + content + rating). Clear instructions: Summarize in 2â€“3 sentences. Mention both positive and negative aspects. Avoid revealing any personal identifiers. Keep it neutral, not promotional. Call Together client with the constructed prompt. Parse response and display in the UI. UI integration On each result card, add: â€œReview summaryâ€ button. On click: Show loader/spinner while LLM call is in progress. Cache summary per (podcast_id) in session_state to avoid repeated calls. Show summary under the button or inside the â€œWhy this?â€ panel (e.g., under a â€œWhat other listeners sayâ€ subtitle). Handle API errors gracefully: Show fallback text: â€œCouldnâ€™t load review summary. Please try again later.â€ Interaction with explainability Keep â€œWhy this?â€ panel focused on ranking signals (Interest, Activity, Query, Popularity). â€œReview summaryâ€ can be visually adjacent but conceptually separate: Ranking explains â€œWhy we recommended this.â€ Review summary explains â€œWhat other listeners think.â€ Telemetry for LLM usage Log events when: User clicks â€œReview summaryâ€. LLM call succeeds/fails. Include in telemetry.csv: event_type: e.g., review_summary_opened, review_summary_error session_id user_ts activity item_id (podcast_id or episode_id) optional: llm_latency_ms (if measured; not required) Telemetry (overall) Log to telemetry.csv (append-only) the following events: impression (card shown) click (card clicked or expanded) play (â–¶ Play latest clicked) completion_50 / completion_80 (can be simulated via buttons or pseudo-timers for demo) skip save dislike_reason (with reason code) not_for_activity why_opened (user opened â€œWhy this?â€ explainability panel) review_summary_opened review_summary_error (if any) Each row should include: session_id user_ts (local or UTC timestamp) activity (Commute, Workout+submode, Focus, Sleep) item_id (episode_id or show_id, consistent) event_type extra fields where useful (e.g., reason, duration_window_before/after, Î», etc.) Metrics panel Provide a small metrics section in the UI, showing: Total impressions Number of plays CTR to play (plays / impressions) Average time-to-first-play (since session start or since first impression) Count of â€œReview summaryâ€ opens Explainability Text (per card) 1â€“2 lines max, derived from main signals; examples: â€œCalm interview format matches Focus; overlaps with â€˜ethicsâ€™, â€˜researchâ€™.â€ â€œUpbeat storytelling fits Workout (30â€“45m); similar to your saved Business shows.â€ â€œShort narrative news brief matches your commute and â€˜Premier Leagueâ€™ interest.â€ Implementation Notes Use scikit-learn for TF-IDF and cosine similarity. Cache TF-IDF matrices and user interest representations with st.cache_data / st.session_state appropriately. Keep main ranking deterministic given the same inputs. The only external network calls allowed are: Together LLM API calls for review summarization. Provide clear inline comments in app.py explaining: TF-IDF build and usage. Signal calculations (InterestFit, ActivityFit, QueryMatch, Popularity, DurationFit). MMR diversification logic. Feedback handling and session_state updates. Telemetry logging. LLM prompt construction and Together API usage for review summaries. Acceptance Criteria Sleep + empty query: Top-10 has no explicit content. Includes soft_start items. At least 1 novel item when Î» â‰¥ 0.5. Workout + duration [20, 45]: Median ep_duration_min among Top-10 lies inside [20, 45]. Feedback â€œToo longâ€: Clicking ğŸ‘ with reason â€œToo longâ€ shifts subsequent suggestions toward shorter episodes (in the same session). â€œWhy this?â€ panel: Shows interest terms, activity keywords, and query hits without errors for each item. LLM review summary: â€œReview summaryâ€ button loads and displays a concise summary of user reviews for the selected podcast, using reviews.json and Together API. Handles missing reviews or API errors gracefully (no crashes). Summaries reflect both positive and negative feedback when present. Telemetry: telemetry.csv grows with user interactions (impression, play, save, dislike_reason, review_summary_opened, etc.). Metrics panel shows counts and CTR to play, plus time-to-first-play.
